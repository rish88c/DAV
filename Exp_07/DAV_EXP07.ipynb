{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNW478oXfjsQ2cUGCSmhSPC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "ir",
      "display_name": "R"
    },
    "language_info": {
      "name": "R"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rish88c/DAV/blob/main/Exp_07/DAV_EXP07.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Experiment - 7"
      ],
      "metadata": {
        "id": "fRwZqYgfqOS1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aim: Perform the steps involved in Text Analytics in Python & R\n"
      ],
      "metadata": {
        "id": "VoQke-ZMqWyO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task to be performed :\n",
        "* Explore Top-5 Text Analytics Libraries in Python (w.r.t Features & Applications)\n",
        "* Explore Top-5 Text Analytics Libraries in R (w.r.t Features & Applications)\n",
        "* Perform the following experiments using Python & R\n",
        "* Tokenization (Sentence & Word)\n",
        "* Frequency Distribution\n",
        "* Remove stopwords & punctuations\n",
        "* Lexicon Normalization (Stemming, Lemmatization)\n",
        "* Part of Speech tagging\n",
        "* Named Entity Recognization\n",
        "* Scrape data from a website"
      ],
      "metadata": {
        "id": "YTkKbKkTqXdS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Scrape data from a website"
      ],
      "metadata": {
        "id": "tahLdmA9zS2R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Specify the URL of the webpage to scrape\n",
        "url = 'https://toscrape.com/'\n",
        "x=''\n",
        "# Send an HTTP GET request to the URL\n",
        "response = requests.get(url)\n",
        "\n",
        "# Check if the request was successful (status code 200)\n",
        "if response.status_code == 200:\n",
        "    # Parse the HTML content of the webpage\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Find and extract specific elements or data from the webpage\n",
        "    # Example: Extract all <p> tags\n",
        "    paragraphs = soup.find_all('p')\n",
        "\n",
        "    # Print the extracted data\n",
        "    for paragraph in paragraphs:\n",
        "        print(paragraph.text)\n",
        "        x += paragraph.text\n",
        "else:\n",
        "    print('Failed to retrieve the webpage. Status code:', response.status_code)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sEqo1ySEqVCI",
        "outputId": "72452ad1-53bd-4280-fad5-c2e3f0082fe3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A fictional bookstore that desperately wants to be scraped. It's a safe place for beginners learning web scraping and for developers validating their scraping technologies as well. Available at: books.toscrape.com\n",
            "A website that lists quotes from famous people. It has many endpoints showing the quotes in many different ways, each of them including new scraping challenges for you, as described below.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TOKENIZER"
      ],
      "metadata": {
        "id": "qZMeTrUC0Lv_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Python\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "nltk.download('punkt')\n",
        "text = x\n",
        "sentences = sent_tokenize(text)\n",
        "words = word_tokenize(text)\n",
        "print(\"Sentences:\", sentences)\n",
        "print(\"Words:\", words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtRal0ue0Mdn",
        "outputId": "b3695510-824b-423d-c948-04f9f0fee8a2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentences: ['A fictional bookstore that desperately wants to be scraped.', \"It's a safe place for beginners learning web scraping and for developers validating their scraping technologies as well.\", 'Available at: books.toscrape.comA website that lists quotes from famous people.', 'It has many endpoints showing the quotes in many different ways, each of them including new scraping challenges for you, as described below.']\n",
            "Words: ['A', 'fictional', 'bookstore', 'that', 'desperately', 'wants', 'to', 'be', 'scraped', '.', 'It', \"'s\", 'a', 'safe', 'place', 'for', 'beginners', 'learning', 'web', 'scraping', 'and', 'for', 'developers', 'validating', 'their', 'scraping', 'technologies', 'as', 'well', '.', 'Available', 'at', ':', 'books.toscrape.comA', 'website', 'that', 'lists', 'quotes', 'from', 'famous', 'people', '.', 'It', 'has', 'many', 'endpoints', 'showing', 'the', 'quotes', 'in', 'many', 'different', 'ways', ',', 'each', 'of', 'them', 'including', 'new', 'scraping', 'challenges', 'for', 'you', ',', 'as', 'described', 'below', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Frequency Distribution"
      ],
      "metadata": {
        "id": "g5dQow4r0TQm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Python\n",
        "from nltk.probability import FreqDist\n",
        "\n",
        "# Assuming 'words' is a list of words\n",
        "fdist = FreqDist(words)\n",
        "\n",
        "# Print frequency counts for each word\n",
        "for word, frequency in fdist.items():\n",
        "    print(f\"{word}: {frequency}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jEuHbSv00T_S",
        "outputId": "d7cd9a05-b03e-4f01-e771-e1a7506449f3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A: 1\n",
            "fictional: 1\n",
            "bookstore: 1\n",
            "that: 2\n",
            "desperately: 1\n",
            "wants: 1\n",
            "to: 1\n",
            "be: 1\n",
            "scraped: 1\n",
            ".: 4\n",
            "It: 2\n",
            "'s: 1\n",
            "a: 1\n",
            "safe: 1\n",
            "place: 1\n",
            "for: 3\n",
            "beginners: 1\n",
            "learning: 1\n",
            "web: 1\n",
            "scraping: 3\n",
            "and: 1\n",
            "developers: 1\n",
            "validating: 1\n",
            "their: 1\n",
            "technologies: 1\n",
            "as: 2\n",
            "well: 1\n",
            "Available: 1\n",
            "at: 1\n",
            ":: 1\n",
            "books.toscrape.comA: 1\n",
            "website: 1\n",
            "lists: 1\n",
            "quotes: 2\n",
            "from: 1\n",
            "famous: 1\n",
            "people: 1\n",
            "has: 1\n",
            "many: 2\n",
            "endpoints: 1\n",
            "showing: 1\n",
            "the: 1\n",
            "in: 1\n",
            "different: 1\n",
            "ways: 1\n",
            ",: 2\n",
            "each: 1\n",
            "of: 1\n",
            "them: 1\n",
            "including: 1\n",
            "new: 1\n",
            "challenges: 1\n",
            "you: 1\n",
            "described: 1\n",
            "below: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Remove stopwords & punctuations\n"
      ],
      "metadata": {
        "id": "lrwbiVGu0Xf5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Python\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_words = [word for word in words if word.lower() not in stop_words and word not in string.punctuation]\n",
        "print(\"Filtered Words:\", filtered_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4QVq3us0aXi",
        "outputId": "77f5a530-760e-4045-e963-02b96b0f174e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered Words: ['fictional', 'bookstore', 'desperately', 'wants', 'scraped', \"'s\", 'safe', 'place', 'beginners', 'learning', 'web', 'scraping', 'developers', 'validating', 'scraping', 'technologies', 'well', 'Available', 'books.toscrape.comA', 'website', 'lists', 'quotes', 'famous', 'people', 'many', 'endpoints', 'showing', 'quotes', 'many', 'different', 'ways', 'including', 'new', 'scraping', 'challenges', 'described']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Lexicon Normalization (Stemming, Lemmatization)"
      ],
      "metadata": {
        "id": "pmlFcyga0isV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "\n",
        "# Download WordNet corpus\n",
        "nltk.download('wordnet')\n",
        "\n",
        "\n",
        "\n",
        "# Tokenize the text and remove punctuation\n",
        "words = [word for word in word_tokenize(text) if word not in string.punctuation]\n",
        "\n",
        "# Initialize stemmer and lemmatizer\n",
        "porter = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Perform stemming and lemmatization\n",
        "stemmed_words = [porter.stem(word) for word in words]\n",
        "lemmatized_words = [lemmatizer.lemmatize(word.lower()) for word in words]\n",
        "\n",
        "# Print the results\n",
        "print(\"Stemmed Words:\", stemmed_words)\n",
        "print(\"Lemmatized Words:\", lemmatized_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbqjxmxa0jnF",
        "outputId": "b0ed71e7-00be-4616-b7b3-d0eb5081cde4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmed Words: ['a', 'fiction', 'bookstor', 'that', 'desper', 'want', 'to', 'be', 'scrape', 'it', \"'s\", 'a', 'safe', 'place', 'for', 'beginn', 'learn', 'web', 'scrape', 'and', 'for', 'develop', 'valid', 'their', 'scrape', 'technolog', 'as', 'well', 'avail', 'at', 'books.toscrape.coma', 'websit', 'that', 'list', 'quot', 'from', 'famou', 'peopl', 'it', 'ha', 'mani', 'endpoint', 'show', 'the', 'quot', 'in', 'mani', 'differ', 'way', 'each', 'of', 'them', 'includ', 'new', 'scrape', 'challeng', 'for', 'you', 'as', 'describ', 'below']\n",
            "Lemmatized Words: ['a', 'fictional', 'bookstore', 'that', 'desperately', 'want', 'to', 'be', 'scraped', 'it', \"'s\", 'a', 'safe', 'place', 'for', 'beginner', 'learning', 'web', 'scraping', 'and', 'for', 'developer', 'validating', 'their', 'scraping', 'technology', 'a', 'well', 'available', 'at', 'books.toscrape.coma', 'website', 'that', 'list', 'quote', 'from', 'famous', 'people', 'it', 'ha', 'many', 'endpoint', 'showing', 'the', 'quote', 'in', 'many', 'different', 'way', 'each', 'of', 'them', 'including', 'new', 'scraping', 'challenge', 'for', 'you', 'a', 'described', 'below']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Part of Speech tagging"
      ],
      "metadata": {
        "id": "5K05djU70q77"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "pos_tags = nltk.pos_tag(filtered_words)\n",
        "print(\"Part of Speech Tags:\", pos_tags)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kd64xl3l0sXR",
        "outputId": "729e0182-971e-45a6-e531-addfafa8d828"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Part of Speech Tags: [('fictional', 'JJ'), ('bookstore', 'NN'), ('desperately', 'RB'), ('wants', 'VBZ'), ('scraped', 'VBD'), (\"'s\", 'POS'), ('safe', 'JJ'), ('place', 'NN'), ('beginners', 'NNS'), ('learning', 'VBG'), ('web', 'JJ'), ('scraping', 'VBG'), ('developers', 'NNS'), ('validating', 'VBG'), ('scraping', 'NN'), ('technologies', 'NNS'), ('well', 'RB'), ('Available', 'NNP'), ('books.toscrape.comA', 'NN'), ('website', 'NN'), ('lists', 'NNS'), ('quotes', 'VBZ'), ('famous', 'JJ'), ('people', 'NNS'), ('many', 'JJ'), ('endpoints', 'NNS'), ('showing', 'VBG'), ('quotes', 'NNS'), ('many', 'JJ'), ('different', 'JJ'), ('ways', 'NNS'), ('including', 'VBG'), ('new', 'JJ'), ('scraping', 'NN'), ('challenges', 'NNS'), ('described', 'VBD')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Named Entity Recognization"
      ],
      "metadata": {
        "id": "9v8Mfmkc0wba"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Python\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "text2=\"New York City, often simply referred to as New York, is the most populous city in the United States. It is located in the northeastern region of the country and is known for its iconic landmarks such as the Statue of Liberty, Times Square, and Central Park. The city is a major hub for finance, culture, and entertainment, attracting millions of tourists every year. Some of the world's leading companies and institutions are headquartered in New York City, making it a global center for commerce and innovation.\"\n",
        "doc = nlp(text2)\n",
        "entities = [(entity.text, entity.label_) for entity in doc.ents]\n",
        "print(\"Named Entities:\", entities)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0k3V6cw40znu",
        "outputId": "a41c713d-05cc-4932-aaea-d7dde3db62a3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Named Entities: [('New York City', 'GPE'), ('New York', 'GPE'), ('the United States', 'GPE'), ('the Statue of Liberty', 'FAC'), ('Times Square', 'FAC'), ('Central Park', 'LOC'), ('millions', 'CARDINAL'), ('New York City', 'GPE')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## IN R"
      ],
      "metadata": {
        "id": "J9_7lD6N04IV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TOKENIZER"
      ],
      "metadata": {
        "id": "cUdA1Kjc09J8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "install.packages(\"tokenizers\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZBxV3jS1JDt",
        "outputId": "2e132831-77dc-4de7-aec7-95ec9258a69b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "also installing the dependencies ‘Rcpp’, ‘SnowballC’\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "library(tokenizers)\n",
        "\n",
        "text <- readline(prompt = \"Enter text: \")\n",
        "\n",
        "word_tokens <- unlist(tokenize_words(text))\n",
        "sentence_tokens <- unlist(tokenize_sentences(text))\n",
        "\n",
        "cat(\"\\nTokenized words:\\n\")\n",
        "print(word_tokens)\n",
        "\n",
        "cat(\"\\nTokenized sentences:\\n\")\n",
        "print(sentence_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3wqPmno04wb",
        "outputId": "729bfb17-6467-456b-dc12-d91d6dc4fe7c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter text: A fictional bookstore that desperately wants to be scraped. It's a safe place for beginners learning web scraping and for developers validating their scraping technologies as well.\n",
            "\n",
            "Tokenized words:\n",
            " [1] \"a\"            \"fictional\"    \"bookstore\"    \"that\"         \"desperately\" \n",
            " [6] \"wants\"        \"to\"           \"be\"           \"scraped\"      \"it's\"        \n",
            "[11] \"a\"            \"safe\"         \"place\"        \"for\"          \"beginners\"   \n",
            "[16] \"learning\"     \"web\"          \"scraping\"     \"and\"          \"for\"         \n",
            "[21] \"developers\"   \"validating\"   \"their\"        \"scraping\"     \"technologies\"\n",
            "[26] \"as\"           \"well\"        \n",
            "\n",
            "Tokenized sentences:\n",
            "[1] \"A fictional bookstore that desperately wants to be scraped.\"                                                             \n",
            "[2] \"It's a safe place for beginners learning web scraping and for developers validating their scraping technologies as well.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Frequency Distribution"
      ],
      "metadata": {
        "id": "BKmHosNS1oZw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_freq <- table(word_tokens)\n",
        "\n",
        "print(\"Most common words:\")\n",
        "print(head(sort(word_freq, decreasing = TRUE), 2))\n",
        "\n",
        "print(\"Frequency of each word:\")\n",
        "print(word_freq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZuEJuiX1rt3",
        "outputId": "bc6ec621-4a3c-4372-c3cf-011562b148aa"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] \"Most common words:\"\n",
            "word_tokens\n",
            "  a for \n",
            "  2   2 \n",
            "[1] \"Frequency of each word:\"\n",
            "word_tokens\n",
            "           a          and           as           be    beginners    bookstore \n",
            "           2            1            1            1            1            1 \n",
            " desperately   developers    fictional          for         it's     learning \n",
            "           1            1            1            2            1            1 \n",
            "       place         safe      scraped     scraping technologies         that \n",
            "           1            1            1            2            1            1 \n",
            "       their           to   validating        wants          web         well \n",
            "           1            1            1            1            1            1 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Remove stopwords & punctuations\n",
        "\n"
      ],
      "metadata": {
        "id": "UtWoH63E1uac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "install.packages(\"tm\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vsaLNRzL14Nc",
        "outputId": "e90c7f0a-2684-481b-c160-92250871f9f9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "also installing the dependencies ‘NLP’, ‘slam’, ‘BH’\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "library(tm)\n",
        "\n",
        "filtered_tokens <- word_tokens[!word_tokens %in% stopwords(\"en\")]\n",
        "\n",
        "print(\"Filtered Tokens:\")\n",
        "print(filtered_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bry-mWV61uHu",
        "outputId": "a924b8cd-38ea-4122-ee99-d7ba80792b42"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading required package: NLP\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] \"Filtered Tokens:\"\n",
            " [1] \"fictional\"    \"bookstore\"    \"desperately\"  \"wants\"        \"scraped\"     \n",
            " [6] \"safe\"         \"place\"        \"beginners\"    \"learning\"     \"web\"         \n",
            "[11] \"scraping\"     \"developers\"   \"validating\"   \"scraping\"     \"technologies\"\n",
            "[16] \"well\"        \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Lexicon Normalization (Stemming, Lemmatization)"
      ],
      "metadata": {
        "id": "lutvE2OG2PpU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stemming <- function(text) {\n",
        "  corpus <- Corpus(VectorSource(text))\n",
        "  corpus <- tm_map(corpus, stemDocument)\n",
        "  return(corpus)\n",
        "}\n",
        "\n",
        "stemmed_corpus <- stemming(filtered_tokens)\n",
        "\n",
        "print(\"Stemmed Tokens:\")\n",
        "print(unlist(sapply(stemmed_corpus, as.character)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbnrCxCG2Y3p",
        "outputId": "61d46272-11dd-41aa-c4f0-177ee50fdc90"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning message in tm_map.SimpleCorpus(corpus, stemDocument):\n",
            "“transformation drops documents”\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] \"Stemmed Tokens:\"\n",
            " [1] \"fiction\"   \"bookstor\"  \"desper\"    \"want\"      \"scrape\"    \"safe\"     \n",
            " [7] \"place\"     \"beginn\"    \"learn\"     \"web\"       \"scrape\"    \"develop\"  \n",
            "[13] \"valid\"     \"scrape\"    \"technolog\" \"well\"     \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "install.packages(\"textstem\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uRnZ6KS72poR",
        "outputId": "aeee6399-3143-4585-e50f-8b9a120f05fe"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "also installing the dependencies ‘zoo’, ‘dtt’, ‘ISOcodes’, ‘sylly.en’, ‘sylly’, ‘syuzhet’, ‘fastmatch’, ‘RcppParallel’, ‘stopwords’, ‘RcppArmadillo’, ‘english’, ‘mgsub’, ‘qdapRegex’, ‘koRpus.lang.en’, ‘hunspell’, ‘koRpus’, ‘lexicon’, ‘quanteda’, ‘textclean’, ‘textshape’\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "library(textstem)\n",
        "lemmatization <- function(text) {\n",
        "  corpus <- Corpus(VectorSource(text))\n",
        "  corpus <- tm_map(corpus, lemmatize_strings)\n",
        "  return(corpus)\n",
        "}\n",
        "\n",
        "lemmatized_corpus <- lemmatization(text)\n",
        "\n",
        "print(\"Lemmatized Tokens:\")\n",
        "print(unlist(sapply(lemmatized_corpus, as.character)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Yn1uj_l3fYX",
        "outputId": "6c4d4d81-baaa-4f49-dcf1-3df0259991a6"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading required package: koRpus.lang.en\n",
            "\n",
            "Loading required package: koRpus\n",
            "\n",
            "Loading required package: sylly\n",
            "\n",
            "For information on available language packages for 'koRpus', run\n",
            "\n",
            "  available.koRpus.lang()\n",
            "\n",
            "and see ?install.koRpus.lang()\n",
            "\n",
            "\n",
            "\n",
            "Attaching package: ‘koRpus’\n",
            "\n",
            "\n",
            "The following object is masked from ‘package:tm’:\n",
            "\n",
            "    readTagged\n",
            "\n",
            "\n",
            "Warning message in tm_map.SimpleCorpus(corpus, lemmatize_strings):\n",
            "“transformation drops documents”\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] \"Lemmatized Tokens:\"\n",
            "[1] \"A fictional bookstore that desperately want to be scrape. It's a safe place for beginner learn web scrape and for developer validate their scrape technology as good.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Scrape data from a website"
      ],
      "metadata": {
        "id": "0cjlSsba2_T2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install and load required libraries\n",
        "install.packages(\"rvest\")\n",
        "library(rvest)\n",
        "\n",
        "# Function to scrape text within <p> tags from a website\n",
        "scrape_website <- function(url) {\n",
        "  webpage <- read_html(url)\n",
        "  paragraphs <- html_nodes(webpage, \"p\")  # Select only <p> tags\n",
        "  text <- html_text(paragraphs)\n",
        "  return(text)\n",
        "}\n",
        "\n",
        "# URL of the website to scrape\n",
        "url <- \"https://toscrape.com/\"\n",
        "\n",
        "# Scrape data from the website\n",
        "paragraphs_text <- scrape_website(url)\n",
        "\n",
        "# Print the scraped text within <p> tags\n",
        "cat(paragraphs_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CK-oiLwu3AKL",
        "outputId": "2b387e86-8311-4284-e003-d47dccace26b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A fictional bookstore that desperately wants to be scraped. It's a safe place for beginners learning web scraping and for developers validating their scraping technologies as well. Available at: books.toscrape.com A website that lists quotes from famous people. It has many endpoints showing the quotes in many different ways, each of them including new scraping challenges for you, as described below."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conclusion - Successfully performed various steps of text analytics in R and Python"
      ],
      "metadata": {
        "id": "PNRlSFe-31Cx"
      }
    }
  ]
}